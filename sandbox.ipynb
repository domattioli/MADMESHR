{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow tensorflow-probability gym matplotlib numpy","metadata":{"trusted":true,"jupyter":{"outputs_hidden":true},"execution":{"iopub.status.busy":"2025-04-13T21:02:34.022938Z","iopub.execute_input":"2025-04-13T21:02:34.023336Z","iopub.status.idle":"2025-04-13T21:02:37.274222Z","shell.execute_reply.started":"2025-04-13T21:02:34.023314Z","shell.execute_reply":"2025-04-13T21:02:37.273323Z"},"collapsed":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: tensorflow-probability in /usr/local/lib/python3.11/dist-packages (0.25.0)\nRequirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.5.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (4.4.2)\nRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (3.1.1)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (0.1.9)\nRequirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability) (25.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"# trainer.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom typing import Optional, Dict\n\nfrom tqdm import trange\nimport matplotlib.pyplot as plt\n\nfrom src.SAC import *             \nfrom src.MeshEnvironment import * \n\n__all__ = ['SACTrainer']\n\nclass SACTrainer:\n    def __init__(self, env, agent, replay_buffer,\n                 total_timesteps=100_000,\n                 batch_size=256,\n                 initial_random_steps=10_000,\n                 eval_interval=5_000,\n                 max_ep_len=100):\n        self.env = env\n        self.agent = agent\n        self.replay_buffer = replay_buffer\n        \n        self.total_timesteps = total_timesteps\n        self.batch_size = batch_size\n        self.initial_random_steps = initial_random_steps\n        self.eval_interval = eval_interval\n        self.max_ep_len = max_ep_len\n\n        self.results = {\n            'timesteps': [],\n            'episode_returns': [],\n            'actor_losses': [],\n            'critic_losses': [],\n            'alpha_values': []\n        }\n\n    def train(self):\n        state, _ = self.env.reset()\n        episode_return, episode_length = 0, 0\n\n        for t in range(1, self.total_timesteps + 1):\n            if t < self.initial_random_steps:\n                action = self.env.action_space.sample()\n            else:\n                action = self.agent.select_action(state)\n\n            next_state, reward, done, truncated, _ = self.env.step(action)\n            self.replay_buffer.add(state, action, reward, next_state, done)\n\n            episode_return += reward\n            episode_length += 1\n            state = next_state\n\n            if done or truncated or episode_length >= self.max_ep_len:\n                print(f\"Episode Return: {episode_return:.2f} | Elements: {len(self.env.elements)}\")\n                self.results['timesteps'].append(t)\n                self.results['episode_returns'].append(episode_return)\n                state, _ = self.env.reset()\n                episode_return, episode_length = 0, 0\n\n            if self.replay_buffer.size() >= self.batch_size and t >= self.initial_random_steps:\n                batch = self.replay_buffer.sample(self.batch_size)\n                train_info = self.agent.train_step(batch)\n\n                if t % 1000 == 0:\n                    self.results['actor_losses'].append(train_info['actor_loss'])\n                    self.results['critic_losses'].append(\n                        (train_info['critic_1_loss'] + train_info['critic_2_loss']) / 2\n                    )\n                    self.results['alpha_values'].append(train_info['alpha'])\n                    print(f\"Timestep: {t} | Actor Loss: {train_info['actor_loss']:.4f} | \"\n                          f\"Critic Loss: {(train_info['critic_1_loss'] + train_info['critic_2_loss']) / 2:.4f} | \"\n                          f\"Alpha: {train_info['alpha']:.4f}\")\n\n            if t % self.eval_interval == 0:\n                eval_return = self.evaluate()\n                print(f\"Evaluation at timestep {t}: {eval_return:.2f}\")\n\n        return self.results\n\n    def evaluate(self, num_episodes=5, render=False):\n        returns = []\n        for _ in range(num_episodes):\n            state, _ = self.env.reset()\n            done, ep_len, total = False, 0, 0\n            while not done and ep_len < self.max_ep_len:\n                action = self.agent.select_action(state, evaluate=True)\n                state, reward, done, truncated, _ = self.env.step(action)\n                total += reward\n                ep_len += 1\n                if render: self.env.render()\n                if truncated: break\n            returns.append(total)\n        return np.mean(returns)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-13T23:10:13.723457Z","iopub.execute_input":"2025-04-13T23:10:13.724375Z","iopub.status.idle":"2025-04-13T23:10:17.802179Z","shell.execute_reply.started":"2025-04-13T23:10:13.724337Z","shell.execute_reply":"2025-04-13T23:10:17.801419Z"}},"outputs":[{"name":"stderr","text":"2025-04-13 23:10:14.025096: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744585814.047386      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744585814.056594      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"# MeshEnvironment.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\nimport gym\nfrom gym import spaces\nfrom collections import deque\nimport random\nimport os\nfrom typing import List, Tuple, Dict, Optional, Union, Any\n\n__all__ = ['MeshEnvironment']\n\nclass MeshEnvironment(gym.Env):\n    def __init__(self, initial_boundary=None, interior_points=None):\n        # Initialize domain boundary\n        if initial_boundary is None:\n            self.initial_boundary = np.array([\n                [-1, -1], [1, -1], [1, 1], [-1, 1]\n            ])\n        else:\n            self.initial_boundary = initial_boundary\n            \n        # Interior points (optional)\n        self.interior_points = interior_points if interior_points is not None else np.array([[0, 0]])\n        \n        # Initialize other properties\n        self.boundary = self.initial_boundary.copy()\n        self.elements = []\n        self.element_qualities = []\n        self.original_area = self._calculate_polygon_area(self.initial_boundary)\n        \n        # Parameters\n        self.n_rv = 2\n        self.g = 3\n        self.beta = 6\n        \n        # Create initial state\n        temp_state = self._get_state_initial()\n        state_size = temp_state.shape[0]\n        \n        # Define action and observation spaces\n        self.action_space = spaces.Box(\n            low=-1, high=1, shape=(3,), dtype=np.float32\n        )\n        \n        self.observation_space = spaces.Box(\n            low=-2, high=2, shape=(state_size,), dtype=np.float32\n        )\n        \n        # Reset the environment\n        self.reset()\n    \n    def reset(self, seed=None):\n        super().reset(seed=seed)\n        self.boundary = self.initial_boundary.copy()\n        self.elements = []\n        self.element_qualities = []\n        state = self._get_state()\n        return state, {}\n    \n    def step(self, action):\n        # Convert normalized action to actual values\n        action_type = int((action[0] + 1) * 1.5)  # Map [-1,1] to [0,3)\n        action_type = min(2, max(0, action_type))  # Clip to [0,2]\n        \n        # Get reference vertex\n        reference_vertex = self._select_reference_vertex()\n        \n        # Convert to cartesian coordinates relative to reference\n        radius = self._calculate_fan_shape_radius(reference_vertex)\n        angle = (action[1] + 1) * np.pi  # Maps [-1,1] to [0,2π]\n        distance = (action[2] + 1) / 2 * radius  # Maps [-1,1] to [0,radius]\n        \n        # Calculate new vertex position\n        new_vertex = reference_vertex + np.array([\n            distance * np.cos(angle),\n            distance * np.sin(angle)\n        ])\n        \n        # Try to form a quad element\n        new_element, valid = self._form_element(reference_vertex, action_type, new_vertex)\n        \n        if not valid:\n            # Invalid element formation\n            return self._get_state(), -0.1, False, False, {\"valid\": False}\n        \n        # Add element to mesh\n        self.elements.append(new_element)\n        quality = self._calculate_element_quality(new_element)\n        self.element_qualities.append(quality)\n        \n        # Update boundary\n        self._update_boundary(new_element)\n        \n        # Check if meshing complete\n        remaining_area = self._calculate_polygon_area(self.boundary)\n        area_ratio = remaining_area / self.original_area\n        \n        if len(self.boundary) <= 4 and self._is_quadrilateral(self.boundary):\n            # Meshing complete\n            return self._get_state(), 10.0, True, False, {\"complete\": True}\n        \n        # Calculate reward\n        reward = self._calculate_reward(new_element, quality, area_ratio)\n        return self._get_state(), reward, False, False, {\"valid\": True}\n    \n    def _get_state_initial(self):\n        \"\"\"Initial state vector of fixed size that doesn't reference observation_space\"\"\"\n        state_components = []\n        \n        # Process surrounding vertices (2 on each side, 3 components each)\n        for _ in range(4):\n            state_components.extend([0, 0, 0])\n        \n        # Process fan-shaped area points (3 points, 3 components each)\n        for _ in range(3):\n            state_components.extend([0, 0, 1])\n        \n        # Add area ratio\n        state_components.append(1.0)\n        \n        return np.array(state_components, dtype=np.float32)\n    \n    def _get_state(self):\n        \"\"\"Create state representation from current boundary\"\"\"\n        if len(self.boundary) < 3:\n            return np.zeros(22, dtype=np.float32)\n        \n        # Select reference vertex\n        reference_vertex = self._select_reference_vertex()\n        ref_idx = -1\n        for i, v in enumerate(self.boundary):\n            if np.array_equal(v, reference_vertex):\n                ref_idx = i\n                break\n        \n        if ref_idx == -1:\n            return np.zeros(22, dtype=np.float32)\n        \n        # Get surrounding vertices\n        surrounding_vertices = []\n        for i in range(1, self.n_rv + 1):\n            left_idx = (ref_idx - i) % len(self.boundary)\n            right_idx = (ref_idx + i) % len(self.boundary)\n            \n            surrounding_vertices.append(self.boundary[left_idx])\n            surrounding_vertices.append(self.boundary[right_idx])\n        \n        # Calculate fan-shaped area points\n        radius = self._calculate_fan_shape_radius(reference_vertex)\n        fan_points = self._get_fan_shape_points(reference_vertex, radius)\n        \n        # Calculate remaining area ratio\n        remaining_area = self._calculate_polygon_area(self.boundary)\n        area_ratio = remaining_area / self.original_area\n        \n        # Build state vector\n        state_components = []\n        \n        # Process surrounding vertices\n        for vertex in surrounding_vertices:\n            rel_vector = vertex - reference_vertex\n            distance = np.linalg.norm(rel_vector)\n            angle = np.arctan2(rel_vector[1], rel_vector[0])\n            \n            # Normalize and add to state\n            norm_distance = distance / radius\n            norm_angle = angle / np.pi\n            \n            state_components.extend([norm_distance, norm_angle, 0])\n        \n        # Process fan-shaped area points\n        for point in fan_points:\n            rel_vector = point - reference_vertex\n            distance = np.linalg.norm(rel_vector)\n            angle = np.arctan2(rel_vector[1], rel_vector[0])\n            \n            # Normalize and add to state\n            norm_distance = distance / radius\n            norm_angle = angle / np.pi\n            \n            state_components.extend([norm_distance, norm_angle, 1])\n        \n        # Add area ratio\n        state_components.append(area_ratio)\n        \n        # Ensure fixed length\n        result = np.array(state_components, dtype=np.float32)\n        if len(result) < 22:\n            result = np.pad(result, (0, 22 - len(result)))\n        elif len(result) > 22:\n            result = result[:22]\n            \n        return result\n    \n    def _calculate_polygon_area(self, polygon):\n        \"\"\"Calculate area of a polygon using the Shoelace formula\"\"\"\n        if len(polygon) < 3:\n            return 0\n            \n        area = 0.0\n        for i in range(len(polygon)):\n            j = (i + 1) % len(polygon)\n            area += polygon[i][0] * polygon[j][1]\n            area -= polygon[j][0] * polygon[i][1]\n            \n        area = abs(area) / 2.0\n        return area\n    \n    def _calculate_element_quality(self, element):\n        \"\"\"Calculate element quality as per Pan et al. Equation 7\"\"\"\n        # Extract edges\n        edges = []\n        for i in range(len(element)):\n            edges.append(element[(i+1) % len(element)] - element[i])\n            \n        # Calculate edge lengths\n        edge_lengths = [np.linalg.norm(edge) for edge in edges]\n        min_edge_length = min(edge_lengths)\n        \n        # Calculate diagonals\n        diag1 = element[2] - element[0]\n        diag2 = element[3] - element[1]\n        diag_lengths = [np.linalg.norm(diag1), np.linalg.norm(diag2)]\n        max_diag_length = max(diag_lengths)\n        \n        # Calculate edge quality (q_edge)\n        q_edge = np.sqrt(2) * min_edge_length / max_diag_length\n        \n        # Calculate angles\n        angles = []\n        for i in range(len(element)):\n            prev = (i - 1) % len(element)\n            next = (i + 1) % len(element)\n            \n            v1 = element[prev] - element[i]\n            v2 = element[next] - element[i]\n            \n            # Normalize vectors\n            v1_norm = v1 / max(1e-10, np.linalg.norm(v1))\n            v2_norm = v2 / max(1e-10, np.linalg.norm(v2))\n            \n            # Calculate angle in degrees\n            dot_product = np.clip(np.dot(v1_norm, v2_norm), -1.0, 1.0)\n            angle = np.arccos(dot_product) * 180 / np.pi\n            angles.append(angle)\n            \n        # Calculate angle quality (q_angle)\n        min_angle = min(angles)\n        max_angle = max(angles)\n        q_angle = min_angle / max_angle\n        \n        # Overall quality\n        quality = np.sqrt(q_edge * q_angle)\n        \n        return quality\n    \n    def _calculate_reward(self, new_element, element_quality, area_ratio):\n        \"\"\"Calculate reward (Equations 5-9 in Pan et al.)\"\"\"\n        # Element quality component\n        eta_e = element_quality\n        \n        # Boundary quality component (simplified)\n        eta_b = -0.2\n        \n        # Density component\n        element_area = self._calculate_polygon_area(new_element)\n        A_min = 0.01 * self.original_area\n        A_max = 0.1 * self.original_area\n        \n        if element_area < A_min:\n            mu = -1\n        elif element_area < A_max:\n            mu = (element_area - A_min) / (A_max - A_min)\n        else:\n            mu = 0\n            \n        # Overall reward\n        reward = eta_e + eta_b + mu\n        \n        return reward\n    \n    def _select_reference_vertex(self):\n        \"\"\"Select the reference vertex with minimum angle\"\"\"\n        if len(self.boundary) <= 2:\n            return self.boundary[0]\n        \n        min_avg_angle = float('inf')\n        ref_vertex_idx = 0\n        \n        for i in range(len(self.boundary)):\n            angles = []\n            for j in range(1, min(self.n_rv + 1, len(self.boundary))):\n                left_idx = (i - j) % len(self.boundary)\n                right_idx = (i + j) % len(self.boundary)\n                \n                left_v = self.boundary[left_idx]\n                center_v = self.boundary[i]\n                right_v = self.boundary[right_idx]\n                \n                v1 = left_v - center_v\n                v2 = right_v - center_v\n                \n                v1_norm = v1 / max(1e-10, np.linalg.norm(v1))\n                v2_norm = v2 / max(1e-10, np.linalg.norm(v2))\n                \n                dot_product = np.clip(np.dot(v1_norm, v2_norm), -1.0, 1.0)\n                angle = np.arccos(dot_product)\n                angles.append(angle)\n            \n            avg_angle = np.mean(angles)\n            if avg_angle < min_avg_angle:\n                min_avg_angle = avg_angle\n                ref_vertex_idx = i\n        \n        return self.boundary[ref_vertex_idx]\n    \n    def _calculate_fan_shape_radius(self, reference_vertex):\n        \"\"\"Calculate radius for fan-shaped area\"\"\"\n        if len(self.boundary) < 3:\n            return 0.5\n        \n        ref_idx = -1\n        for i, v in enumerate(self.boundary):\n            if np.array_equal(v, reference_vertex):\n                ref_idx = i\n                break\n        \n        if ref_idx == -1:\n            return 0.5\n        \n        edge_lengths = []\n        for j in range(min(self.n_rv, len(self.boundary) - 1)):\n            left_idx = (ref_idx - j - 1) % len(self.boundary)\n            right_idx = (ref_idx + j + 1) % len(self.boundary)\n            \n            left_edge = np.linalg.norm(self.boundary[left_idx] - self.boundary[(left_idx+1) % len(self.boundary)])\n            right_edge = np.linalg.norm(self.boundary[right_idx] - self.boundary[(right_idx-1) % len(self.boundary)])\n            \n            edge_lengths.extend([left_edge, right_edge])\n        \n        L = np.mean(edge_lengths) if edge_lengths else 0.5\n        return self.beta * L\n    \n    def _get_fan_shape_points(self, reference_vertex, radius):\n        \"\"\"Get points in fan-shaped areas\"\"\"\n        fan_points = []\n        \n        # Find reference vertex in boundary\n        left_idx, right_idx = -1, -1\n        for i, v in enumerate(self.boundary):\n            if np.array_equal(v, reference_vertex):\n                left_idx = (i - 1) % len(self.boundary)\n                right_idx = (i + 1) % len(self.boundary)\n                break\n        \n        if left_idx == -1:\n            angles = np.linspace(0, 2*np.pi, self.g+1)[:-1]\n            for angle in angles:\n                fan_points.append(reference_vertex + radius * np.array([np.cos(angle), np.sin(angle)]))\n            return fan_points\n        \n        # Calculate angle between left and right vertices\n        left_v = self.boundary[left_idx] - reference_vertex\n        right_v = self.boundary[right_idx] - reference_vertex\n        \n        left_angle = np.arctan2(left_v[1], left_v[0])\n        right_angle = np.arctan2(right_v[1], right_v[0])\n        \n        # Ensure right angle is ahead of left angle\n        if right_angle < left_angle:\n            right_angle += 2 * np.pi\n            \n        angles = np.linspace(left_angle, right_angle, self.g+2)[1:-1]\n        \n        for angle in angles:\n            direction = np.array([np.cos(angle), np.sin(angle)])\n            \n            # Check if interior point is in this direction\n            closest_point = None\n            min_distance = radius\n            \n            for point in self.interior_points:\n                to_point = point - reference_vertex\n                projection = np.dot(to_point, direction)\n                \n                if projection <= 0 or projection > radius:\n                    continue\n                \n                perp_dist = np.linalg.norm(to_point - projection * direction)\n                \n                if perp_dist < 0.1 * radius:\n                    distance = np.linalg.norm(to_point)\n                    if distance < min_distance:\n                        min_distance = distance\n                        closest_point = point\n            \n            if closest_point is None:\n                closest_point = reference_vertex + radius * direction\n            \n            fan_points.append(closest_point)\n        \n        return fan_points\n    \n    def _form_element(self, reference_vertex, action_type, new_vertex):\n        \"\"\"Form a quadrilateral element based on action type\"\"\"\n        ref_idx = -1\n        for i, v in enumerate(self.boundary):\n            if np.array_equal(v, reference_vertex):\n                ref_idx = i\n                break\n        \n        if ref_idx == -1:\n            return None, False\n        \n        if action_type == 0:\n            # Use existing vertices\n            if len(self.boundary) < 4:\n                return None, False\n            \n            v1 = reference_vertex\n            v2 = self.boundary[(ref_idx + 1) % len(self.boundary)]\n            v3 = self.boundary[(ref_idx + 2) % len(self.boundary)]\n            v4 = self.boundary[(ref_idx - 1) % len(self.boundary)]\n            \n        elif action_type == 1:\n            # Add one new vertex\n            if len(self.boundary) < 3:\n                return None, False\n            \n            v1 = reference_vertex\n            v2 = self.boundary[(ref_idx + 1) % len(self.boundary)]\n            v3 = new_vertex\n            v4 = self.boundary[(ref_idx - 1) % len(self.boundary)]\n            \n        else:\n            # Not fully implemented - type 2 would add two vertices\n            return None, False\n        \n        element = np.array([v1, v2, v3, v4])\n        \n        # Validate the element\n        if not self._is_valid_quad(element):\n            return None, False\n        \n        return element, True\n    \n    def _is_valid_quad(self, quad):\n        \"\"\"Check if quadrilateral is valid\"\"\"\n        # Check for self-intersections\n        edges = [\n            (quad[0], quad[1]),\n            (quad[1], quad[2]),\n            (quad[2], quad[3]),\n            (quad[3], quad[0])\n        ]\n        \n        for i in range(len(edges)):\n            for j in range(i+2, len(edges)):\n                if i == 0 and j == 3:\n                    continue\n                if self._do_segments_intersect(edges[i][0], edges[i][1], edges[j][0], edges[j][1]):\n                    return False\n        \n        # Check orientation\n        return self._is_convex_quad(quad)\n    \n    def _is_convex_quad(self, quad):\n        \"\"\"Check if quadrilateral is convex\"\"\"\n        for i in range(4):\n            prev = (i - 1) % 4\n            curr = i\n            next = (i + 1) % 4\n            \n            v1 = quad[prev] - quad[curr]\n            v2 = quad[next] - quad[curr]\n            \n            cross_z = v1[0] * v2[1] - v1[1] * v2[0]\n            if cross_z <= 0:\n                return False\n                \n        return True\n    \n    def _do_segments_intersect(self, p1, p2, p3, p4):\n        \"\"\"Check if two line segments intersect\"\"\"\n        def orientation(p, q, r):\n            val = (q[1] - p[1]) * (r[0] - q[0]) - (q[0] - p[0]) * (r[1] - q[1])\n            if val == 0:\n                return 0\n            return 1 if val > 0 else 2\n        \n        def on_segment(p, q, r):\n            return (q[0] <= max(p[0], r[0]) and q[0] >= min(p[0], r[0]) and\n                    q[1] <= max(p[1], r[1]) and q[1] >= min(p[1], r[1]))\n        \n        o1 = orientation(p1, p2, p3)\n        o2 = orientation(p1, p2, p4)\n        o3 = orientation(p3, p4, p1)\n        o4 = orientation(p3, p4, p2)\n        \n        if o1 != o2 and o3 != o4:\n            return True\n            \n        if o1 == 0 and on_segment(p1, p3, p2): return True\n        if o2 == 0 and on_segment(p1, p4, p2): return True\n        if o3 == 0 and on_segment(p3, p1, p4): return True\n        if o4 == 0 and on_segment(p3, p2, p4): return True\n        \n        return False\n    \n    def _update_boundary(self, new_element):\n        \"\"\"Update boundary after adding a new element\"\"\"\n        boundary_list = self.boundary.tolist()\n        element_points = new_element.tolist()\n        \n        # Create list of element edges\n        element_edges = []\n        for i in range(len(element_points)):\n            edge = [element_points[i], element_points[(i+1) % len(element_points)]]\n            edge.sort(key=lambda p: (p[0], p[1]))\n            element_edges.append(edge)\n        \n        # Find edges to remove\n        edges_to_remove = []\n        for i in range(len(boundary_list)):\n            edge = [boundary_list[i], boundary_list[(i+1) % len(boundary_list)]]\n            edge.sort(key=lambda p: (p[0], p[1]))\n            \n            if edge in element_edges:\n                edges_to_remove.append(i)\n        \n        edges_to_remove.sort(reverse=True)\n        \n        # Remove edges\n        for idx in edges_to_remove:\n            boundary_list.pop((idx + 1) % len(boundary_list))\n        \n        # Add new edges\n        for edge in element_edges:\n            edge_in_boundary = False\n            for i in range(len(boundary_list)):\n                b_edge = [boundary_list[i], boundary_list[(i+1) % len(boundary_list)]]\n                b_edge.sort(key=lambda p: (p[0], p[1]))\n                \n                if edge == b_edge:\n                    edge_in_boundary = True\n                    break\n            \n            if not edge_in_boundary:\n                for i in range(len(boundary_list)):\n                    if boundary_list[i] == edge[0] or boundary_list[i] == edge[1]:\n                        boundary_list.insert(i+1, edge[1] if boundary_list[i] == edge[0] else edge[0])\n                        break\n        \n        self.boundary = np.array(boundary_list)\n    \n    def _is_quadrilateral(self, polygon):\n        \"\"\"Check if polygon is a quadrilateral\"\"\"\n        return len(polygon) == 4\n    \n    def plot_domain(self):\n        \"\"\"Plot the initial domain with interior points\"\"\"\n        plt.figure(figsize=(8, 8))\n        \n        # Plot boundary\n        boundary = self.initial_boundary\n        x, y = boundary[:, 0], boundary[:, 1]\n        plt.plot(np.append(x, x[0]), np.append(y, y[0]), 'k-', linewidth=2)\n        \n        # Plot interior points\n        if self.interior_points is not None and len(self.interior_points) > 0:\n            plt.scatter(self.interior_points[:, 0], self.interior_points[:, 1], \n                       color='blue', marker='o', s=100)\n        \n        plt.grid(True)\n        plt.axis('equal')\n        plt.title('Initial Domain with Interior Point')\n        plt.xlabel('X')\n        plt.ylabel('Y')\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T23:10:34.713007Z","iopub.execute_input":"2025-04-13T23:10:34.713534Z","iopub.status.idle":"2025-04-13T23:10:34.761423Z","shell.execute_reply.started":"2025-04-13T23:10:34.713509Z","shell.execute_reply":"2025-04-13T23:10:34.760851Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"# SAC.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\nimport gym\nfrom gym import spaces\nfrom collections import deque\nimport random\nimport os\nfrom typing import List, Tuple, Dict, Optional, Union, Any\n\n\n__all__ = ['ReplayBuffer', 'SAC']\n\n\nclass ReplayBuffer:\n    \"\"\"Experience replay buffer for SAC.\"\"\"\n    \n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n    \n    def add(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n    \n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n        return states, actions, rewards, next_states, dones\n    \n    def size(self):\n        return len(self.buffer)\n\n\nclass SAC:\n    def __init__(self, state_dim, action_dim, hidden_dim=128, gamma=0.99, tau=0.005):\n        self.gamma = gamma\n        self.tau = tau\n        self.alpha = 1.0\n        self.action_dim = action_dim\n        \n        # Build networks\n        self.actor = self._build_actor(state_dim, action_dim, hidden_dim)\n        self.critic_1 = self._build_critic(state_dim, action_dim, hidden_dim)\n        self.critic_2 = self._build_critic(state_dim, action_dim, hidden_dim)\n        \n        # Target networks\n        self.target_critic_1 = self._build_critic(state_dim, action_dim, hidden_dim)\n        self.target_critic_2 = self._build_critic(state_dim, action_dim, hidden_dim)\n        self.target_critic_1.set_weights(self.critic_1.get_weights())\n        self.target_critic_2.set_weights(self.critic_2.get_weights())\n        \n        # Optimizers\n        self.actor_optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n        self.critic_1_optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n        self.critic_2_optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n        \n        # Alpha\n        self.target_entropy = -action_dim\n        self.log_alpha = tf.Variable(tf.math.log(self.alpha), dtype=tf.float32)\n        self.alpha_optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n    \n    def _build_actor(self, state_dim, action_dim, hidden_dim):\n        inputs = layers.Input(shape=(state_dim,))\n        x = layers.Dense(hidden_dim, activation='relu')(inputs)\n        x = layers.Dense(hidden_dim, activation='relu')(x)\n        \n        # Output mean and log_std separately\n        mean = layers.Dense(action_dim, activation='tanh')(x)\n        log_std = layers.Dense(action_dim)(x)\n        log_std = layers.Lambda(lambda x: tf.clip_by_value(x, -20, 2))(log_std)\n        \n        model = keras.Model(inputs=inputs, outputs=[mean, log_std])\n        return model\n    \n    def _build_critic(self, state_dim, action_dim, hidden_dim):\n        state_input = layers.Input(shape=(state_dim,))\n        action_input = layers.Input(shape=(action_dim,))\n        \n        x = layers.Concatenate()([state_input, action_input])\n        x = layers.Dense(hidden_dim, activation='relu')(x)\n        x = layers.Dense(hidden_dim, activation='relu')(x)\n        q_value = layers.Dense(1)(x)\n        \n        return keras.Model(inputs=[state_input, action_input], outputs=q_value)\n    \n    def _sample_action(self, state):\n        \"\"\"Sample action and compute log probability\"\"\"\n        mean, log_std = self.actor(state)\n        std = tf.exp(log_std)\n        normal_dist = tfp.distributions.Normal(mean, std)\n        \n        # Sample from normal distribution\n        z = normal_dist.sample()\n        action = tf.tanh(z)\n        \n        # Calculate log probability\n        log_prob = normal_dist.log_prob(z)\n        # Apply tanh squashing correction\n        log_prob = tf.reduce_sum(log_prob - tf.math.log(1 - action**2 + 1e-6), axis=1, keepdims=True)\n        return action, log_prob\n    \n    def select_action(self, state):\n        \"\"\"Select action for evaluation\"\"\"\n        state = np.expand_dims(state, axis=0) if state.ndim == 1 else state\n        \n        # For inference, use mean action (no sampling)\n        mean, _ = self.actor(state)\n        action = tf.tanh(mean)\n        return action.numpy()[0]\n    \n    def train_step(self, batch):\n        states, actions, rewards, next_states, dones = batch\n        \n        # Convert to tensors\n        states = tf.convert_to_tensor(states, dtype=tf.float32)\n        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n        \n        # Get current alpha\n        alpha = tf.exp(self.log_alpha)\n        \n        # Update critics\n        with tf.GradientTape(persistent=True) as tape:\n            # Sample actions from actor for next states\n            next_actions, next_log_probs = self._sample_action(next_states)\n            \n            # Compute target Q-values\n            target_q1 = self.target_critic_1([next_states, next_actions])\n            target_q2 = self.target_critic_2([next_states, next_actions])\n            target_q = tf.minimum(target_q1, target_q2) - alpha * next_log_probs\n            target_q = rewards + (1 - dones) * self.gamma * target_q\n            \n            # Compute current Q-values\n            current_q1 = self.critic_1([states, actions])\n            current_q2 = self.critic_2([states, actions])\n            \n            # Compute critic losses\n            critic_1_loss = tf.reduce_mean((current_q1 - target_q) ** 2)\n            critic_2_loss = tf.reduce_mean((current_q2 - target_q) ** 2)\n        \n        # Update critics\n        critic_1_gradients = tape.gradient(critic_1_loss, self.critic_1.trainable_variables)\n        critic_2_gradients = tape.gradient(critic_2_loss, self.critic_2.trainable_variables)\n        self.critic_1_optimizer.apply_gradients(zip(critic_1_gradients, self.critic_1.trainable_variables))\n        self.critic_2_optimizer.apply_gradients(zip(critic_2_gradients, self.critic_2.trainable_variables))\n        \n        # Update actor\n        with tf.GradientTape() as tape:\n            # Sample actions and log probs\n            actions, log_probs = self._sample_action(states)\n            \n            # Compute Q-values\n            q1 = self.critic_1([states, actions])\n            q2 = self.critic_2([states, actions])\n            q = tf.minimum(q1, q2)\n            \n            # Actor loss\n            actor_loss = tf.reduce_mean(alpha * log_probs - q)\n        \n        actor_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n        self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))\n        \n        # Update alpha\n        with tf.GradientTape() as tape:\n            _, log_probs = self._sample_action(states)\n            alpha_loss = -tf.reduce_mean(self.log_alpha * (log_probs + self.target_entropy))\n        \n        alpha_gradients = tape.gradient(alpha_loss, [self.log_alpha])\n        self.alpha_optimizer.apply_gradients(zip(alpha_gradients, [self.log_alpha]))\n        \n        # Update target networks\n        for target_var, source_var in zip(self.target_critic_1.variables, self.critic_1.variables):\n            target_var.assign(self.tau * source_var + (1 - self.tau) * target_var)\n        for target_var, source_var in zip(self.target_critic_2.variables, self.critic_2.variables):\n            target_var.assign(self.tau * source_var + (1 - self.tau) * target_var)\n        \n        return {\n            'actor_loss': actor_loss.numpy(),\n            'critic_1_loss': critic_1_loss.numpy(),\n            'critic_2_loss': critic_2_loss.numpy(),\n            'alpha': alpha.numpy()\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T23:36:08.319151Z","iopub.execute_input":"2025-04-13T23:36:08.319768Z","iopub.status.idle":"2025-04-13T23:36:08.339019Z","shell.execute_reply.started":"2025-04-13T23:36:08.319745Z","shell.execute_reply":"2025-04-13T23:36:08.338430Z"}},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":"# Visualization.py","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n__all__ =['plot_training_results', 'visualize_mesh_generation']\n\ndef plot_training_results(results):\n    \"\"\"Plot training metrics over time.\"\"\"\n    plt.figure(figsize=(15, 10))\n\n    # Episode returns\n    plt.subplot(2, 2, 1)\n    plt.plot(results['timesteps'], results['episode_returns'], label=\"Episode Return\")\n    plt.xlabel('Timesteps')\n    plt.ylabel('Return')\n    plt.title('Episode Return Over Time')\n    plt.grid(True)\n\n    # Actor loss\n    plt.subplot(2, 2, 2)\n    plt.plot(results['actor_losses'], label=\"Actor Loss\", color='orange')\n    plt.xlabel('Updates (x1000)')\n    plt.ylabel('Loss')\n    plt.title('Actor Loss')\n    plt.grid(True)\n\n    # Critic loss\n    plt.subplot(2, 2, 3)\n    plt.plot(results['critic_losses'], label=\"Critic Loss\", color='green')\n    plt.xlabel('Updates (x1000)')\n    plt.ylabel('Loss')\n    plt.title('Critic Loss')\n    plt.grid(True)\n\n    # Alpha values\n    plt.subplot(2, 2, 4)\n    plt.plot(results['alpha_values'], label=\"Alpha\", color='red')\n    plt.xlabel('Updates (x1000)')\n    plt.ylabel('Alpha')\n    plt.title('Alpha Value')\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.savefig('training_results.png')\n    plt.show()\n\n\ndef visualize_mesh_generation(env, agent, max_steps=100):\n    \"\"\"\n    Visualize mesh generation by the agent over time.\n    Saves a 'mesh_generation.gif' of the process if imageio is installed.\n    \"\"\"\n    import warnings\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n\n    state, _ = env.reset()\n    frames = []\n\n    # Initial frame\n    fig = env.render()\n    frames.append(fig)\n\n    for step in range(max_steps):\n        action = agent.select_action(state, evaluate=True)\n        next_state, reward, done, truncated, _ = env.step(action)\n\n        fig = env.render()\n        frames.append(fig)\n\n        state = next_state\n        if done or truncated:\n            break\n\n    try:\n        import imageio\n        from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n\n        images = []\n        for fig in frames:\n            canvas = FigureCanvas(fig)\n            canvas.draw()\n            img = np.array(canvas.renderer.buffer_rgba())\n            images.append(img)\n            plt.close(fig)\n\n        imageio.mimsave('mesh_generation.gif', images, fps=2)\n        print(\"✅ Saved: mesh_generation.gif\")\n\n    except ImportError:\n        print(\"⚠️ imageio not installed — skipping animation export.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T23:15:27.231766Z","iopub.execute_input":"2025-04-13T23:15:27.232539Z","iopub.status.idle":"2025-04-13T23:15:27.246314Z","shell.execute_reply.started":"2025-04-13T23:15:27.232508Z","shell.execute_reply":"2025-04-13T23:15:27.245500Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"def train(env, agent, replay_buffer, \n          total_timesteps=100000, \n          batch_size=256,\n          initial_random_steps=10000,\n          eval_interval=5000, \n          max_ep_len=100):\n    \"\"\"Train the agent using SAC.\"\"\"\n    episode_return = 0\n    episode_length = 0\n    state, _ = env.reset()\n    \n    results = {\n        'timesteps': [],\n        'episode_returns': [],\n        'actor_losses': [],\n        'critic_losses': [],\n        'alpha_values': []\n    }\n    \n    for t in range(1, total_timesteps + 1):\n        # Sample action\n        if t < initial_random_steps:\n            action = env.action_space.sample()\n        else:\n            action = agent.select_action(state)\n        \n        # Take step\n        next_state, reward, done, truncated, info = env.step(action)\n        episode_return += reward\n        episode_length += 1\n        \n        # Store transition\n        replay_buffer.add(state, action, reward, next_state, done)\n        \n        # Move to next state\n        state = next_state\n        \n        # Reset if episode ends\n        if done or truncated or episode_length >= max_ep_len:\n            print(f\"Episode Return: {episode_return:.2f} | Elements: {len(env.elements)}\")\n            results['timesteps'].append(t)\n            results['episode_returns'].append(episode_return)\n            \n            state, _ = env.reset()\n            episode_return = 0\n            episode_length = 0\n        \n        # Train agent\n        if replay_buffer.size() >= batch_size and t >= initial_random_steps:\n            batch = replay_buffer.sample(batch_size)\n            train_info = agent.train_step(batch)\n            \n            if t % 1000 == 0:\n                results['actor_losses'].append(train_info['actor_loss'])\n                results['critic_losses'].append((train_info['critic_1_loss'] + train_info['critic_2_loss'])/2)\n                results['alpha_values'].append(train_info['alpha'])\n                \n                print(f\"Timestep: {t}/{total_timesteps} | \"\n                      f\"Actor Loss: {train_info['actor_loss']:.4f} | \"\n                      f\"Critic Loss: {(train_info['critic_1_loss'] + train_info['critic_2_loss'])/2:.4f} | \"\n                      f\"Alpha: {train_info['alpha']:.4f}\")\n        \n        # Evaluate\n        if t % eval_interval == 0:\n            eval_return = evaluate(env, agent, num_episodes=5)\n            print(f\"Evaluation at timestep {t}: {eval_return:.2f}\")\n            \n    return results\n\ndef evaluate(env, agent, num_episodes=5, render=False, max_ep_len=100):\n    \"\"\"Evaluate the agent.\"\"\"\n    returns = []\n    \n    for _ in range(num_episodes):\n        state, _ = env.reset()\n        episode_return = 0\n        episode_length = 0\n        done = False\n        \n        while not done and episode_length < max_ep_len:\n            action = agent.select_action(state, evaluate=True)\n            next_state, reward, done, truncated, _ = env.step(action)\n            episode_return += reward\n            episode_length += 1\n            \n            if render:\n                env.render()\n                \n            state = next_state\n            if truncated:\n                break\n                \n        returns.append(episode_return)\n        \n    return np.mean(returns)\n    \n\ndef main():\n    env = MeshEnvironment()\n    env.plot_domain()\n    \n    print(f\"Observation space: {env.observation_space.shape}\")\n    agent = SAC(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0], hidden_dim=128)\n    replay_buffer = ReplayBuffer(capacity=100_000)\n    \n    state, _ = env.reset()\n    for t in range(1, 5000):\n        action = env.action_space.sample() if t < 1000 else agent.select_action(state)\n        next_state, reward, done, _, info = env.step(action)\n        replay_buffer.add(state, action, reward, next_state, done)\n        state = next_state\n        if done: state, _ = env.reset()\n        \n        if replay_buffer.size() >= 64 and t >= 1000 and t % 10 == 0:\n            try:\n                batch = replay_buffer.sample(64)\n                train_info = agent.train_step(batch)\n                if t % 500 == 0: print(f\"Step {t}: Loss={train_info['actor_loss']:.4f}\")\n            except Exception as e:\n                print(f\"Training error at step {t}: {str(e)}\")\n                break\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T23:36:13.237108Z","iopub.execute_input":"2025-04-13T23:36:13.237777Z","iopub.status.idle":"2025-04-13T23:38:09.013825Z","shell.execute_reply.started":"2025-04-13T23:36:13.237754Z","shell.execute_reply":"2025-04-13T23:38:09.013035Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Observation space: (22,)\nStep 1000: Loss=-1.7832\nStep 1500: Loss=-3.9451\nStep 2000: Loss=-4.2066\nStep 2500: Loss=-4.5623\nStep 3000: Loss=-4.8318\nStep 3500: Loss=-5.2009\nStep 4000: Loss=-5.4751\nStep 4500: Loss=-5.8283\n","output_type":"stream"}],"execution_count":54}]}