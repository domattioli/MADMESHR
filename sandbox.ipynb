{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow tensorflow-probability gym matplotlib numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T02:35:52.443355Z","iopub.execute_input":"2025-04-14T02:35:52.443935Z","iopub.status.idle":"2025-04-14T02:35:55.536665Z","shell.execute_reply.started":"2025-04-14T02:35:52.443914Z","shell.execute_reply":"2025-04-14T02:35:55.535951Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: tensorflow-probability in /usr/local/lib/python3.11/dist-packages (0.25.0)\nRequirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.1)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.5.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (4.4.2)\nRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (3.1.1)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability) (0.1.9)\nRequirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability) (25.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# trainer.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom typing import Optional, Dict\n\nfrom tqdm import trange\nimport matplotlib.pyplot as plt\n\n# from src.SAC import *             \n# from src.MeshEnvironment import * \n\n__all__ = ['SACTrainer']\n\nclass SACTrainer:\n    def __init__(self, env, agent, replay_buffer,\n                 total_timesteps=100_000,\n                 batch_size=256,\n                 initial_random_steps=10_000,\n                 eval_interval=5_000,\n                 max_ep_len=100):\n        self.env = env\n        self.agent = agent\n        self.replay_buffer = replay_buffer\n        \n        self.total_timesteps = total_timesteps\n        self.batch_size = batch_size\n        self.initial_random_steps = initial_random_steps\n        self.eval_interval = eval_interval\n        self.max_ep_len = max_ep_len\n\n        self.results = {\n            'timesteps': [],\n            'episode_returns': [],\n            'actor_losses': [],\n            'critic_losses': [],\n            'alpha_values': []\n        }\n\n    def train(self):\n        state, _ = self.env.reset()\n        episode_return, episode_length = 0, 0\n\n        for t in range(1, self.total_timesteps + 1):\n            if t < self.initial_random_steps:\n                action = self.env.action_space.sample()\n            else:\n                action = self.agent.select_action(state)\n\n            next_state, reward, done, truncated, _ = self.env.step(action)\n            self.replay_buffer.add(state, action, reward, next_state, done)\n\n            episode_return += reward\n            episode_length += 1\n            state = next_state\n\n            if done or truncated or episode_length >= self.max_ep_len:\n                print(f\"Episode Return: {episode_return:.2f} | Elements: {len(self.env.elements)}\")\n                self.results['timesteps'].append(t)\n                self.results['episode_returns'].append(episode_return)\n                state, _ = self.env.reset()\n                episode_return, episode_length = 0, 0\n\n            if self.replay_buffer.size() >= self.batch_size and t >= self.initial_random_steps:\n                batch = self.replay_buffer.sample(self.batch_size)\n                train_info = self.agent.train_step(batch)\n\n                if t % 1000 == 0:\n                    self.results['actor_losses'].append(train_info['actor_loss'])\n                    self.results['critic_losses'].append(\n                        (train_info['critic_1_loss'] + train_info['critic_2_loss']) / 2\n                    )\n                    self.results['alpha_values'].append(train_info['alpha'])\n                    print(f\"Timestep: {t} | Actor Loss: {train_info['actor_loss']:.4f} | \"\n                          f\"Critic Loss: {(train_info['critic_1_loss'] + train_info['critic_2_loss']) / 2:.4f} | \"\n                          f\"Alpha: {train_info['alpha']:.4f}\")\n\n            if t % self.eval_interval == 0:\n                eval_return = self.evaluate()\n                print(f\"Evaluation at timestep {t}: {eval_return:.2f}\")\n\n        return self.results\n\n    def evaluate(self, num_episodes=5, render=False):\n        returns = []\n        for _ in range(num_episodes):\n            state, _ = self.env.reset()\n            done, ep_len, total = False, 0, 0\n            while not done and ep_len < self.max_ep_len:\n                action = self.agent.select_action(state, evaluate=True)\n                state, reward, done, truncated, _ = self.env.step(action)\n                total += reward\n                ep_len += 1\n                if render: self.env.render()\n                if truncated: break\n            returns.append(total)\n        return np.mean(returns)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-14T02:35:55.538232Z","iopub.execute_input":"2025-04-14T02:35:55.538884Z","iopub.status.idle":"2025-04-14T02:35:55.551487Z","shell.execute_reply.started":"2025-04-14T02:35:55.538850Z","shell.execute_reply":"2025-04-14T02:35:55.550747Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# MeshEnvironment.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\nimport gym\nfrom gym import spaces\nfrom collections import deque\nimport random\nimport os\nfrom typing import List, Tuple, Dict, Optional, Union, Any\n\n__all__ = ['MeshEnvironment']\n\nclass MeshEnvironment(gym.Env):\n    def __init__(self, initial_boundary=None, interior_points=None):\n        # Initialize domain boundary\n        if initial_boundary is None:\n            self.initial_boundary = np.array([\n                [-1, -1], [1, -1], [1, 1], [-1, 1]\n            ])\n        else:\n            self.initial_boundary = initial_boundary\n            \n        # Interior points (optional)\n        self.interior_points = interior_points if interior_points is not None else np.array([[0, 0]])\n        \n        # Initialize other properties\n        self.boundary = self.initial_boundary.copy()\n        self.elements = []\n        self.element_qualities = []\n        self.original_area = self._calculate_polygon_area(self.initial_boundary)\n        \n        # Parameters\n        self.n_rv = 2\n        self.g = 3\n        self.beta = 6\n        \n        # Create initial state\n        temp_state = self._get_state_initial()\n        state_size = temp_state.shape[0]\n        \n        # Define action and observation spaces\n        self.action_space = spaces.Box(\n            low=-1, high=1, shape=(3,), dtype=np.float32\n        )\n        \n        self.observation_space = spaces.Box(\n            low=-2, high=2, shape=(state_size,), dtype=np.float32\n        )\n        \n        # Reset the environment\n        self.reset()\n    \n    def reset(self, seed=None):\n        super().reset(seed=seed)\n        self.boundary = self.initial_boundary.copy()\n        self.elements = []\n        self.element_qualities = []\n        state = self._get_state()\n        return state, {}\n    \n    def step(self, action):\n        # Convert normalized action to actual values\n        action_type = int((action[0] + 1) * 1.5)  # Map [-1,1] to [0,3)\n        action_type = min(2, max(0, action_type))  # Clip to [0,2]\n        \n        # Get reference vertex\n        reference_vertex = self._select_reference_vertex()\n        \n        # Convert to cartesian coordinates relative to reference\n        radius = self._calculate_fan_shape_radius(reference_vertex)\n        angle = (action[1] + 1) * np.pi  # Maps [-1,1] to [0,2π]\n        distance = (action[2] + 1) / 2 * radius  # Maps [-1,1] to [0,radius]\n        \n        # Calculate new vertex position\n        new_vertex = reference_vertex + np.array([\n            distance * np.cos(angle),\n            distance * np.sin(angle)\n        ])\n        \n        # Try to form a quad element\n        new_element, valid = self._form_element(reference_vertex, action_type, new_vertex)\n        \n        if not valid:\n            # Invalid element formation\n            return self._get_state(), -0.1, False, False, {\"valid\": False}\n        \n        # Add element to mesh\n        self.elements.append(new_element)\n        quality = self._calculate_element_quality(new_element)\n        self.element_qualities.append(quality)\n        \n        # Update boundary\n        self._update_boundary(new_element)\n        \n        # Check if meshing complete\n        remaining_area = self._calculate_polygon_area(self.boundary)\n        area_ratio = remaining_area / self.original_area\n        \n        if len(self.boundary) <= 4 and self._is_quadrilateral(self.boundary):\n            # Meshing complete\n            return self._get_state(), 10.0, True, False, {\"complete\": True}\n        \n        # Calculate reward\n        reward = self._calculate_reward(new_element, quality, area_ratio)\n        return self._get_state(), reward, False, False, {\"valid\": True}\n    \n    def _get_state_initial(self):\n        \"\"\"Initial state vector of fixed size that doesn't reference observation_space\"\"\"\n        state_components = []\n        \n        # Process surrounding vertices (2 on each side, 3 components each)\n        for _ in range(4):\n            state_components.extend([0, 0, 0])\n        \n        # Process fan-shaped area points (3 points, 3 components each)\n        for _ in range(3):\n            state_components.extend([0, 0, 1])\n        \n        # Add area ratio\n        state_components.append(1.0)\n        \n        return np.array(state_components, dtype=np.float32)\n    \n    def _get_state(self):\n        \"\"\"Create state representation from current boundary\"\"\"\n        if len(self.boundary) < 3:\n            return np.zeros(22, dtype=np.float32)\n        \n        # Select reference vertex\n        reference_vertex = self._select_reference_vertex()\n        ref_idx = -1\n        for i, v in enumerate(self.boundary):\n            if np.array_equal(v, reference_vertex):\n                ref_idx = i\n                break\n        \n        if ref_idx == -1:\n            return np.zeros(22, dtype=np.float32)\n        \n        # Get surrounding vertices\n        surrounding_vertices = []\n        for i in range(1, self.n_rv + 1):\n            left_idx = (ref_idx - i) % len(self.boundary)\n            right_idx = (ref_idx + i) % len(self.boundary)\n            \n            surrounding_vertices.append(self.boundary[left_idx])\n            surrounding_vertices.append(self.boundary[right_idx])\n        \n        # Calculate fan-shaped area points\n        radius = self._calculate_fan_shape_radius(reference_vertex)\n        fan_points = self._get_fan_shape_points(reference_vertex, radius)\n        \n        # Calculate remaining area ratio\n        remaining_area = self._calculate_polygon_area(self.boundary)\n        area_ratio = remaining_area / self.original_area\n        \n        # Build state vector\n        state_components = []\n        \n        # Process surrounding vertices\n        for vertex in surrounding_vertices:\n            rel_vector = vertex - reference_vertex\n            distance = np.linalg.norm(rel_vector)\n            angle = np.arctan2(rel_vector[1], rel_vector[0])\n            \n            # Normalize and add to state\n            norm_distance = distance / radius\n            norm_angle = angle / np.pi\n            \n            state_components.extend([norm_distance, norm_angle, 0])\n        \n        # Process fan-shaped area points\n        for point in fan_points:\n            rel_vector = point - reference_vertex\n            distance = np.linalg.norm(rel_vector)\n            angle = np.arctan2(rel_vector[1], rel_vector[0])\n            \n            # Normalize and add to state\n            norm_distance = distance / radius\n            norm_angle = angle / np.pi\n            \n            state_components.extend([norm_distance, norm_angle, 1])\n        \n        # Add area ratio\n        state_components.append(area_ratio)\n        \n        # Ensure fixed length\n        result = np.array(state_components, dtype=np.float32)\n        if len(result) < 22:\n            result = np.pad(result, (0, 22 - len(result)))\n        elif len(result) > 22:\n            result = result[:22]\n            \n        return result\n    \n    def _calculate_polygon_area(self, polygon):\n        \"\"\"Calculate area of a polygon using the Shoelace formula\"\"\"\n        if len(polygon) < 3:\n            return 0\n            \n        area = 0.0\n        for i in range(len(polygon)):\n            j = (i + 1) % len(polygon)\n            area += polygon[i][0] * polygon[j][1]\n            area -= polygon[j][0] * polygon[i][1]\n            \n        area = abs(area) / 2.0\n        return area\n    \n    def _calculate_element_quality(self, element):\n        \"\"\"Calculate element quality as per Pan et al. Equation 7\"\"\"\n        # Extract edges\n        edges = []\n        for i in range(len(element)):\n            edges.append(element[(i+1) % len(element)] - element[i])\n            \n        # Calculate edge lengths\n        edge_lengths = [np.linalg.norm(edge) for edge in edges]\n        min_edge_length = min(edge_lengths)\n        \n        # Calculate diagonals\n        diag1 = element[2] - element[0]\n        diag2 = element[3] - element[1]\n        diag_lengths = [np.linalg.norm(diag1), np.linalg.norm(diag2)]\n        max_diag_length = max(diag_lengths)\n        \n        # Calculate edge quality (q_edge)\n        q_edge = np.sqrt(2) * min_edge_length / max_diag_length\n        \n        # Calculate angles\n        angles = []\n        for i in range(len(element)):\n            prev = (i - 1) % len(element)\n            next = (i + 1) % len(element)\n            \n            v1 = element[prev] - element[i]\n            v2 = element[next] - element[i]\n            \n            # Normalize vectors\n            v1_norm = v1 / max(1e-10, np.linalg.norm(v1))\n            v2_norm = v2 / max(1e-10, np.linalg.norm(v2))\n            \n            # Calculate angle in degrees\n            dot_product = np.clip(np.dot(v1_norm, v2_norm), -1.0, 1.0)\n            angle = np.arccos(dot_product) * 180 / np.pi\n            angles.append(angle)\n            \n        # Calculate angle quality (q_angle)\n        min_angle = min(angles)\n        max_angle = max(angles)\n        q_angle = min_angle / max_angle\n        \n        # Overall quality\n        quality = np.sqrt(q_edge * q_angle)\n        \n        return quality\n    \n    def _calculate_reward(self, new_element, element_quality, area_ratio):\n        \"\"\"Calculate reward for creating a new element\n        \n        This function computes the reward based on:\n        1. Element quality (higher quality = higher reward)\n        2. Boundary quality (how well the new boundary maintains good angles)\n        3. Progress toward completing the mesh\n        4. Element density appropriateness\n        \n        Args:\n            new_element: The new quadrilateral element\n            element_quality: Quality of the new element (0-1)\n            area_ratio: Ratio of remaining area to original area\n            \n        Returns:\n            Computed reward value\n        \"\"\"\n        # 1. Element quality component (0-1)\n        eta_e = element_quality\n        \n        # 2. Boundary quality component\n        # Calculate angles at vertices of the remaining boundary\n        boundary_angles = []\n        for i in range(len(self.boundary)):\n            prev = (i - 1) % len(self.boundary)\n            next = (i + 1) % len(self.boundary)\n            \n            v1 = self.boundary[prev] - self.boundary[i]\n            v2 = self.boundary[next] - self.boundary[i]\n            \n            v1_norm = v1 / max(1e-10, np.linalg.norm(v1))\n            v2_norm = v2 / max(1e-10, np.linalg.norm(v2))\n            \n            dot_product = np.clip(np.dot(v1_norm, v2_norm), -1.0, 1.0)\n            angle = np.arccos(dot_product) * 180 / np.pi\n            boundary_angles.append(angle)\n        \n        # Penalize very small angles (encourage more regular boundaries)\n        min_angle = min(boundary_angles) if boundary_angles else 90.0\n        max_angle = max(boundary_angles) if boundary_angles else 90.0\n        \n        # Normalized boundary quality (-1 to 0), with 0 being best\n        target_angle = 90.0  # ideal angle\n        min_angle_penalty = max(0, (target_angle - min_angle) / target_angle)\n        max_angle_penalty = max(0, (max_angle - target_angle) / (180.0 - target_angle))\n        eta_b = -0.5 * (min_angle_penalty + max_angle_penalty)\n        \n        # 3. Progress component (0-1) - reward finishing the mesh\n        progress_reward = 0.0\n        if len(self.boundary) <= 6:  # Getting close to completion\n            progress_reward = (1.0 - area_ratio) * 0.5  # Scale by how much has been meshed\n        \n        # 4. Element density component\n        element_area = self._calculate_polygon_area(new_element)\n        avg_area = self.original_area / max(len(self.elements), 1) \n        optimal_area = avg_area * (1.0 - 0.2 * area_ratio)  # Target smaller elements toward the end\n        \n        # Penalty for elements that are too large or too small\n        area_ratio = element_area / optimal_area\n        if area_ratio < 0.5 or area_ratio > 2.0:\n            density_penalty = -0.2\n        else:\n            density_penalty = 0.0\n        \n        # Overall reward (weighted components)\n        reward = 1.0 * eta_e + 0.5 * eta_b + progress_reward + density_penalty\n    \n        # Add large bonus for completing the mesh\n        if len(self.boundary) <= 4 and self._is_quadrilateral(self.boundary):\n            reward += 5.0\n        return reward\n    \n    def _select_reference_vertex(self):\n        \"\"\"Select the reference vertex with minimum angle\"\"\"\n        if len(self.boundary) <= 2:\n            return self.boundary[0]\n        \n        min_avg_angle = float('inf')\n        ref_vertex_idx = 0\n        \n        for i in range(len(self.boundary)):\n            angles = []\n            for j in range(1, min(self.n_rv + 1, len(self.boundary))):\n                left_idx = (i - j) % len(self.boundary)\n                right_idx = (i + j) % len(self.boundary)\n                \n                left_v = self.boundary[left_idx]\n                center_v = self.boundary[i]\n                right_v = self.boundary[right_idx]\n                \n                v1 = left_v - center_v\n                v2 = right_v - center_v\n                \n                v1_norm = v1 / max(1e-10, np.linalg.norm(v1))\n                v2_norm = v2 / max(1e-10, np.linalg.norm(v2))\n                \n                dot_product = np.clip(np.dot(v1_norm, v2_norm), -1.0, 1.0)\n                angle = np.arccos(dot_product)\n                angles.append(angle)\n            \n            avg_angle = np.mean(angles)\n            if avg_angle < min_avg_angle:\n                min_avg_angle = avg_angle\n                ref_vertex_idx = i\n        \n        return self.boundary[ref_vertex_idx]\n    \n    def _calculate_fan_shape_radius(self, reference_vertex):\n        \"\"\"Calculate radius for fan-shaped area\"\"\"\n        if len(self.boundary) < 3:\n            return 0.5\n        \n        ref_idx = -1\n        for i, v in enumerate(self.boundary):\n            if np.array_equal(v, reference_vertex):\n                ref_idx = i\n                break\n        \n        if ref_idx == -1:\n            return 0.5\n        \n        edge_lengths = []\n        for j in range(min(self.n_rv, len(self.boundary) - 1)):\n            left_idx = (ref_idx - j - 1) % len(self.boundary)\n            right_idx = (ref_idx + j + 1) % len(self.boundary)\n            \n            left_edge = np.linalg.norm(self.boundary[left_idx] - self.boundary[(left_idx+1) % len(self.boundary)])\n            right_edge = np.linalg.norm(self.boundary[right_idx] - self.boundary[(right_idx-1) % len(self.boundary)])\n            \n            edge_lengths.extend([left_edge, right_edge])\n        \n        L = np.mean(edge_lengths) if edge_lengths else 0.5\n        return self.beta * L\n    \n    def _get_fan_shape_points(self, reference_vertex, radius):\n        \"\"\"Get points in fan-shaped areas\"\"\"\n        fan_points = []\n        \n        # Find reference vertex in boundary\n        left_idx, right_idx = -1, -1\n        for i, v in enumerate(self.boundary):\n            if np.array_equal(v, reference_vertex):\n                left_idx = (i - 1) % len(self.boundary)\n                right_idx = (i + 1) % len(self.boundary)\n                break\n        \n        if left_idx == -1:\n            angles = np.linspace(0, 2*np.pi, self.g+1)[:-1]\n            for angle in angles:\n                fan_points.append(reference_vertex + radius * np.array([np.cos(angle), np.sin(angle)]))\n            return fan_points\n        \n        # Calculate angle between left and right vertices\n        left_v = self.boundary[left_idx] - reference_vertex\n        right_v = self.boundary[right_idx] - reference_vertex\n        \n        left_angle = np.arctan2(left_v[1], left_v[0])\n        right_angle = np.arctan2(right_v[1], right_v[0])\n        \n        # Ensure right angle is ahead of left angle\n        if right_angle < left_angle:\n            right_angle += 2 * np.pi\n            \n        angles = np.linspace(left_angle, right_angle, self.g+2)[1:-1]\n        \n        for angle in angles:\n            direction = np.array([np.cos(angle), np.sin(angle)])\n            \n            # Check if interior point is in this direction\n            closest_point = None\n            min_distance = radius\n            \n            for point in self.interior_points:\n                to_point = point - reference_vertex\n                projection = np.dot(to_point, direction)\n                \n                if projection <= 0 or projection > radius:\n                    continue\n                \n                perp_dist = np.linalg.norm(to_point - projection * direction)\n                \n                if perp_dist < 0.1 * radius:\n                    distance = np.linalg.norm(to_point)\n                    if distance < min_distance:\n                        min_distance = distance\n                        closest_point = point\n            \n            if closest_point is None:\n                closest_point = reference_vertex + radius * direction\n            \n            fan_points.append(closest_point)\n        \n        return fan_points\n    \n    def _form_element(self, reference_vertex, action_type, new_vertex):\n        \"\"\"Form a quadrilateral element based on action type\"\"\"\n        ref_idx = -1\n        for i, v in enumerate(self.boundary):\n            if np.array_equal(v, reference_vertex):\n                ref_idx = i\n                break\n        \n        if ref_idx == -1:\n            return None, False\n        \n        if action_type == 0:\n            # Use existing vertices\n            if len(self.boundary) < 4:\n                return None, False\n            \n            v1 = reference_vertex\n            v2 = self.boundary[(ref_idx + 1) % len(self.boundary)]\n            v3 = self.boundary[(ref_idx + 2) % len(self.boundary)]\n            v4 = self.boundary[(ref_idx - 1) % len(self.boundary)]\n            \n        elif action_type == 1:\n            # Add one new vertex\n            if len(self.boundary) < 3:\n                return None, False\n            \n            v1 = reference_vertex\n            v2 = self.boundary[(ref_idx + 1) % len(self.boundary)]\n            v3 = new_vertex\n            v4 = self.boundary[(ref_idx - 1) % len(self.boundary)]\n            \n        else:\n            # Not fully implemented - type 2 would add two vertices\n            return None, False\n        \n        element = np.array([v1, v2, v3, v4])\n        \n        # Validate the element\n        if not self._is_valid_quad(element):\n            return None, False\n        \n        return element, True\n    \n    def _is_valid_quad(self, quad):\n        \"\"\"Check if quadrilateral is valid\"\"\"\n        # Check for self-intersections\n        edges = [\n            (quad[0], quad[1]),\n            (quad[1], quad[2]),\n            (quad[2], quad[3]),\n            (quad[3], quad[0])\n        ]\n        \n        for i in range(len(edges)):\n            for j in range(i+2, len(edges)):\n                if i == 0 and j == 3:\n                    continue\n                if self._do_segments_intersect(edges[i][0], edges[i][1], edges[j][0], edges[j][1]):\n                    return False\n        \n        # Check orientation\n        return self._is_convex_quad(quad)\n    \n    def _is_convex_quad(self, quad):\n        \"\"\"Check if quadrilateral is convex\"\"\"\n        for i in range(4):\n            prev = (i - 1) % 4\n            curr = i\n            next = (i + 1) % 4\n            \n            v1 = quad[prev] - quad[curr]\n            v2 = quad[next] - quad[curr]\n            \n            cross_z = v1[0] * v2[1] - v1[1] * v2[0]\n            if cross_z <= 0:\n                return False\n                \n        return True\n    \n    def _do_segments_intersect(self, p1, p2, p3, p4):\n        \"\"\"Check if two line segments intersect\"\"\"\n        def orientation(p, q, r):\n            val = (q[1] - p[1]) * (r[0] - q[0]) - (q[0] - p[0]) * (r[1] - q[1])\n            if val == 0:\n                return 0\n            return 1 if val > 0 else 2\n        \n        def on_segment(p, q, r):\n            return (q[0] <= max(p[0], r[0]) and q[0] >= min(p[0], r[0]) and\n                    q[1] <= max(p[1], r[1]) and q[1] >= min(p[1], r[1]))\n        \n        o1 = orientation(p1, p2, p3)\n        o2 = orientation(p1, p2, p4)\n        o3 = orientation(p3, p4, p1)\n        o4 = orientation(p3, p4, p2)\n        \n        if o1 != o2 and o3 != o4:\n            return True\n            \n        if o1 == 0 and on_segment(p1, p3, p2): return True\n        if o2 == 0 and on_segment(p1, p4, p2): return True\n        if o3 == 0 and on_segment(p3, p1, p4): return True\n        if o4 == 0 and on_segment(p3, p2, p4): return True\n        \n        return False\n    \n    def _update_boundary(self, new_element):\n        \"\"\"Update boundary after adding a new element\"\"\"\n        boundary_list = self.boundary.tolist()\n        element_points = new_element.tolist()\n        \n        # Create list of element edges\n        element_edges = []\n        for i in range(len(element_points)):\n            edge = [element_points[i], element_points[(i+1) % len(element_points)]]\n            edge.sort(key=lambda p: (p[0], p[1]))\n            element_edges.append(edge)\n        \n        # Find edges to remove\n        edges_to_remove = []\n        for i in range(len(boundary_list)):\n            edge = [boundary_list[i], boundary_list[(i+1) % len(boundary_list)]]\n            edge.sort(key=lambda p: (p[0], p[1]))\n            \n            if edge in element_edges:\n                edges_to_remove.append(i)\n        \n        edges_to_remove.sort(reverse=True)\n        \n        # Remove edges\n        for idx in edges_to_remove:\n            boundary_list.pop((idx + 1) % len(boundary_list))\n        \n        # Add new edges\n        for edge in element_edges:\n            edge_in_boundary = False\n            for i in range(len(boundary_list)):\n                b_edge = [boundary_list[i], boundary_list[(i+1) % len(boundary_list)]]\n                b_edge.sort(key=lambda p: (p[0], p[1]))\n                \n                if edge == b_edge:\n                    edge_in_boundary = True\n                    break\n            \n            if not edge_in_boundary:\n                for i in range(len(boundary_list)):\n                    if boundary_list[i] == edge[0] or boundary_list[i] == edge[1]:\n                        boundary_list.insert(i+1, edge[1] if boundary_list[i] == edge[0] else edge[0])\n                        break\n        \n        self.boundary = np.array(boundary_list)\n    \n    def _is_quadrilateral(self, polygon):\n        \"\"\"Check if polygon is a quadrilateral\"\"\"\n        return len(polygon) == 4\n    \n    def plot_domain(self):\n        \"\"\"Plot the initial domain with interior points\"\"\"\n        plt.figure(figsize=(8, 8))\n        \n        # Plot boundary\n        boundary = self.initial_boundary\n        x, y = boundary[:, 0], boundary[:, 1]\n        plt.plot(np.append(x, x[0]), np.append(y, y[0]), 'k-', linewidth=2)\n        \n        # Plot interior points\n        if self.interior_points is not None and len(self.interior_points) > 0:\n            plt.scatter(self.interior_points[:, 0], self.interior_points[:, 1], \n                       color='blue', marker='o', s=100)\n        \n        plt.grid(True)\n        plt.axis('equal')\n        plt.title('Initial Domain with Interior Point')\n        plt.xlabel('X')\n        plt.ylabel('Y')\n        plt.show()\n\n\n    def render(self):\n        \"\"\"Render the current state of the environment\"\"\"\n        fig, ax = plt.subplots(figsize=(8, 8))\n        \n        # Plot boundary\n        if len(self.boundary) > 0:\n            x, y = self.boundary[:, 0], self.boundary[:, 1]\n            ax.plot(np.append(x, x[0]), np.append(y, y[0]), 'k-', linewidth=2, label='Current Boundary')\n        \n        # Plot original boundary for reference\n        x, y = self.initial_boundary[:, 0], self.initial_boundary[:, 1]\n        ax.plot(np.append(x, x[0]), np.append(y, y[0]), 'k--', linewidth=1, alpha=0.5, label='Initial Boundary')\n        \n        # Plot interior points\n        if self.interior_points is not None and len(self.interior_points) > 0:\n            ax.scatter(self.interior_points[:, 0], self.interior_points[:, 1], \n                       color='blue', marker='o', s=100, label='Interior Points')\n        \n        # Plot quadrilateral elements\n        for i, element in enumerate(self.elements):\n            x, y = element[:, 0], element[:, 1]\n            quality = self.element_qualities[i] if i < len(self.element_qualities) else 0\n            color = plt.cm.viridis(quality)  # Color based on quality\n            ax.fill(x, y, alpha=0.5, color=color, edgecolor='k')\n            \n            # Add element number for debugging\n            centroid = np.mean(element, axis=0)\n            ax.text(centroid[0], centroid[1], f\"{i}\", fontsize=8, ha='center', va='center')\n        \n        # Add a colorbar\n        sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis)\n        sm.set_array([])\n        cbar = plt.colorbar(sm, ax=ax)\n        cbar.set_label('Element Quality')\n        \n        # Add reference vertex\n        if len(self.boundary) >= 3:\n            ref_vertex = self._select_reference_vertex()\n            radius = self._calculate_fan_shape_radius(ref_vertex)\n            ax.scatter([ref_vertex[0]], [ref_vertex[1]], color='red', marker='*', s=200, label='Reference Vertex')\n            \n            # Draw fan shape radius\n            circle = plt.Circle((ref_vertex[0], ref_vertex[1]), radius, fill=False, color='red', linestyle='--', alpha=0.5)\n            ax.add_patch(circle)\n        \n        ax.set_aspect('equal')\n        ax.grid(True)\n        ax.set_title(f'Mesh Environment: {len(self.elements)} elements')\n        ax.set_xlabel('X')\n        ax.set_ylabel('Y')\n        ax.legend()\n    \n        return fig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T02:35:55.552254Z","iopub.execute_input":"2025-04-14T02:35:55.552454Z","iopub.status.idle":"2025-04-14T02:36:11.257878Z","shell.execute_reply.started":"2025-04-14T02:35:55.552439Z","shell.execute_reply":"2025-04-14T02:36:11.257126Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stderr","text":"2025-04-14 02:35:57.186518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744598157.382342      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744598157.433605      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# SAC.py","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\nimport gym\nfrom gym import spaces\nfrom collections import deque\nimport random\nimport os\nfrom typing import List, Tuple, Dict, Optional, Union, Any\n\n\n__all__ = ['ReplayBuffer', 'SAC']\n\n\nclass ReplayBuffer:\n    \"\"\"Experience replay buffer for SAC.\"\"\"\n    \n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n    \n    def add(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n    \n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n        return states, actions, rewards, next_states, dones\n    \n    def size(self):\n        return len(self.buffer)\n\n\nclass SAC:\n    def __init__(self, state_dim, action_dim, hidden_dim=128, gamma=0.99, tau=0.005):\n        self.gamma = gamma\n        self.tau = tau\n        self.alpha = 1.0\n        self.action_dim = action_dim\n        \n        # Build networks\n        self.actor = self._build_actor(state_dim, action_dim, hidden_dim)\n        self.critic_1 = self._build_critic(state_dim, action_dim, hidden_dim)\n        self.critic_2 = self._build_critic(state_dim, action_dim, hidden_dim)\n        \n        # Target networks\n        self.target_critic_1 = self._build_critic(state_dim, action_dim, hidden_dim)\n        self.target_critic_2 = self._build_critic(state_dim, action_dim, hidden_dim)\n        self.target_critic_1.set_weights(self.critic_1.get_weights())\n        self.target_critic_2.set_weights(self.critic_2.get_weights())\n        \n        # Optimizers\n        self.actor_optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n        self.critic_1_optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n        self.critic_2_optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n        \n        # Alpha\n        self.target_entropy = -action_dim\n        self.log_alpha = tf.Variable(tf.math.log(self.alpha), dtype=tf.float32)\n        self.alpha_optimizer = keras.optimizers.Adam(learning_rate=3e-4)\n    \n    def _build_actor(self, state_dim, action_dim, hidden_dim):\n        inputs = layers.Input(shape=(state_dim,))\n        x = layers.Dense(hidden_dim, activation='relu')(inputs)\n        x = layers.Dense(hidden_dim, activation='relu')(x)\n        \n        # Output mean and log_std separately\n        mean = layers.Dense(action_dim, activation='tanh')(x)\n        log_std = layers.Dense(action_dim)(x)\n        log_std = layers.Lambda(lambda x: tf.clip_by_value(x, -20, 2))(log_std)\n        \n        model = keras.Model(inputs=inputs, outputs=[mean, log_std])\n        return model\n    \n    def _build_critic(self, state_dim, action_dim, hidden_dim):\n        state_input = layers.Input(shape=(state_dim,))\n        action_input = layers.Input(shape=(action_dim,))\n        \n        x = layers.Concatenate()([state_input, action_input])\n        x = layers.Dense(hidden_dim, activation='relu')(x)\n        x = layers.Dense(hidden_dim, activation='relu')(x)\n        q_value = layers.Dense(1)(x)\n        \n        return keras.Model(inputs=[state_input, action_input], outputs=q_value)\n    \n    def _sample_action(self, state):\n        \"\"\"Sample action and compute log probability\"\"\"\n        mean, log_std = self.actor(state)\n        std = tf.exp(log_std)\n        normal_dist = tfp.distributions.Normal(mean, std)\n        \n        # Sample from normal distribution\n        z = normal_dist.sample()\n        action = tf.tanh(z)\n        \n        # Calculate log probability\n        log_prob = normal_dist.log_prob(z)\n        # Apply tanh squashing correction\n        log_prob = tf.reduce_sum(log_prob - tf.math.log(1 - action**2 + 1e-6), axis=1, keepdims=True)\n        return action, log_prob\n    \n    def select_action(self, state, evaluate=False):\n        \"\"\"Select action for evaluation or training\n        \n        Args:\n            state: Current state\n            evaluate: If True, use deterministic action (mean); if False, sample from distribution\n            \n        Returns:\n            Selected action\n        \"\"\"\n        state = np.expand_dims(state, axis=0) if state.ndim == 1 else state\n        \n        if evaluate:\n            # For evaluation, use mean action (no sampling)\n            mean, _ = self.actor(state)\n            action = tf.tanh(mean)\n        else:\n            # For training/exploration, sample from distribution\n            mean, log_std = self.actor(state)\n            std = tf.exp(log_std)\n            normal_dist = tfp.distributions.Normal(mean, std)\n            z = normal_dist.sample()\n            action = tf.tanh(z)\n        \n        return action.numpy()[0]\n    \n    def train_step(self, batch):\n        states, actions, rewards, next_states, dones = batch\n        \n        # Convert to tensors\n        states = tf.convert_to_tensor(states, dtype=tf.float32)\n        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n        \n        # Get current alpha\n        alpha = tf.exp(self.log_alpha)\n        \n        # Update critics\n        with tf.GradientTape(persistent=True) as tape:\n            # Sample actions from actor for next states\n            next_actions, next_log_probs = self._sample_action(next_states)\n            \n            # Compute target Q-values\n            target_q1 = self.target_critic_1([next_states, next_actions])\n            target_q2 = self.target_critic_2([next_states, next_actions])\n            target_q = tf.minimum(target_q1, target_q2) - alpha * next_log_probs\n            target_q = rewards + (1 - dones) * self.gamma * target_q\n            \n            # Compute current Q-values\n            current_q1 = self.critic_1([states, actions])\n            current_q2 = self.critic_2([states, actions])\n            \n            # Compute critic losses\n            critic_1_loss = tf.reduce_mean((current_q1 - target_q) ** 2)\n            critic_2_loss = tf.reduce_mean((current_q2 - target_q) ** 2)\n        \n        # Update critics\n        critic_1_gradients = tape.gradient(critic_1_loss, self.critic_1.trainable_variables)\n        critic_2_gradients = tape.gradient(critic_2_loss, self.critic_2.trainable_variables)\n        self.critic_1_optimizer.apply_gradients(zip(critic_1_gradients, self.critic_1.trainable_variables))\n        self.critic_2_optimizer.apply_gradients(zip(critic_2_gradients, self.critic_2.trainable_variables))\n        \n        # Update actor\n        with tf.GradientTape() as tape:\n            # Sample actions and log probs\n            actions, log_probs = self._sample_action(states)\n            \n            # Compute Q-values\n            q1 = self.critic_1([states, actions])\n            q2 = self.critic_2([states, actions])\n            q = tf.minimum(q1, q2)\n            \n            # Actor loss\n            actor_loss = tf.reduce_mean(alpha * log_probs - q)\n        \n        actor_gradients = tape.gradient(actor_loss, self.actor.trainable_variables)\n        self.actor_optimizer.apply_gradients(zip(actor_gradients, self.actor.trainable_variables))\n        \n        # Update alpha\n        with tf.GradientTape() as tape:\n            _, log_probs = self._sample_action(states)\n            alpha_loss = -tf.reduce_mean(self.log_alpha * (log_probs + self.target_entropy))\n        \n        alpha_gradients = tape.gradient(alpha_loss, [self.log_alpha])\n        self.alpha_optimizer.apply_gradients(zip(alpha_gradients, [self.log_alpha]))\n        \n        # Update target networks\n        for target_var, source_var in zip(self.target_critic_1.variables, self.critic_1.variables):\n            target_var.assign(self.tau * source_var + (1 - self.tau) * target_var)\n        for target_var, source_var in zip(self.target_critic_2.variables, self.critic_2.variables):\n            target_var.assign(self.tau * source_var + (1 - self.tau) * target_var)\n        \n        return {\n            'actor_loss': actor_loss.numpy(),\n            'critic_1_loss': critic_1_loss.numpy(),\n            'critic_2_loss': critic_2_loss.numpy(),\n            'alpha': alpha.numpy()\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T02:36:15.644917Z","iopub.execute_input":"2025-04-14T02:36:15.645698Z","iopub.status.idle":"2025-04-14T02:36:15.669738Z","shell.execute_reply.started":"2025-04-14T02:36:15.645662Z","shell.execute_reply":"2025-04-14T02:36:15.669073Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Visualization.py","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n__all__ =['plot_training_results', 'visualize_mesh_generation']\n\ndef plot_training_results(results):\n    \"\"\"Plot training metrics over time.\"\"\"\n    plt.figure(figsize=(15, 10))\n\n    # Episode returns\n    plt.subplot(2, 2, 1)\n    plt.plot(results['timesteps'], results['episode_returns'], label=\"Episode Return\")\n    plt.xlabel('Timesteps')\n    plt.ylabel('Return')\n    plt.title('Episode Return Over Time')\n    plt.grid(True)\n\n    # Actor loss\n    plt.subplot(2, 2, 2)\n    plt.plot(results['actor_losses'], label=\"Actor Loss\", color='orange')\n    plt.xlabel('Updates (x1000)')\n    plt.ylabel('Loss')\n    plt.title('Actor Loss')\n    plt.grid(True)\n\n    # Critic loss\n    plt.subplot(2, 2, 3)\n    plt.plot(results['critic_losses'], label=\"Critic Loss\", color='green')\n    plt.xlabel('Updates (x1000)')\n    plt.ylabel('Loss')\n    plt.title('Critic Loss')\n    plt.grid(True)\n\n    # Alpha values\n    plt.subplot(2, 2, 4)\n    plt.plot(results['alpha_values'], label=\"Alpha\", color='red')\n    plt.xlabel('Updates (x1000)')\n    plt.ylabel('Alpha')\n    plt.title('Alpha Value')\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.savefig('training_results.png')\n    plt.show()\n\n\ndef visualize_mesh_generation(env, agent, max_steps=100):\n    \"\"\"\n    Visualize mesh generation by the agent over time.\n    Saves a 'mesh_generation.gif' of the process if imageio is installed.\n    \"\"\"\n    import warnings\n    warnings.filterwarnings(\"ignore\", category=UserWarning)\n\n    state, _ = env.reset()\n    frames = []\n\n    # Initial frame\n    fig = env.render()\n    frames.append(fig)\n\n    for step in range(max_steps):\n        action = agent.select_action(state, evaluate=True)\n        next_state, reward, done, truncated, _ = env.step(action)\n\n        fig = env.render()\n        frames.append(fig)\n\n        state = next_state\n        if done or truncated:\n            break\n\n    try:\n        import imageio\n        from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n\n        images = []\n        for fig in frames:\n            canvas = FigureCanvas(fig)\n            canvas.draw()\n            img = np.array(canvas.renderer.buffer_rgba())\n            images.append(img)\n            plt.close(fig)\n\n        imageio.mimsave('mesh_generation.gif', images, fps=2)\n        print(\"✅ Saved: mesh_generation.gif\")\n\n    except ImportError:\n        print(\"⚠️ imageio not installed — skipping animation export.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T02:36:11.281210Z","iopub.execute_input":"2025-04-14T02:36:11.281463Z","iopub.status.idle":"2025-04-14T02:36:11.304101Z","shell.execute_reply.started":"2025-04-14T02:36:11.281435Z","shell.execute_reply":"2025-04-14T02:36:11.303550Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport sys\nimport time\n\n# Add src to path if needed\n# src_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'src')\n# if src_dir not in sys.path:\n#     sys.path.append(src_dir)\n\n# from src.MeshEnvironment import MeshEnvironment\n# from src.SAC import SAC, ReplayBuffer\n# from src.trainer import SACTrainer\n# from src.utils.visualization import plot_training_results\n\n# Create a grid-like training domain\ndef create_grid_domain(width=3, height=3, interior_points=2):\n    \"\"\"Create a grid-like domain with boundary and interior points\"\"\"\n    # Create boundary points (clockwise order)\n    x_vals = np.linspace(-1, 1, width)\n    y_vals = np.linspace(-1, 1, height)\n    \n    boundary = []\n    # Bottom edge\n    for x in x_vals:\n        boundary.append([x, y_vals[0]])\n    # Right edge\n    for y in y_vals[1:]:\n        boundary.append([x_vals[-1], y])\n    # Top edge (reversed)\n    for x in x_vals[-2::-1]:\n        boundary.append([x, y_vals[-1]])\n    # Left edge (reversed)\n    for y in y_vals[-2:0:-1]:\n        boundary.append([x_vals[0], y])\n    \n    boundary = np.array(boundary)\n    \n    # Create interior points\n    if interior_points == 2:\n        interior = np.array([\n            [0, 0],\n            [0.5, -0.5]\n        ])\n    elif interior_points == 3:\n        interior = np.array([\n            [0, 0],\n            [0.5, -0.5],\n            [-0.5, 0.5]\n        ])\n    else:\n        # Default case\n        interior = np.array([[0, 0]])\n    \n    return boundary, interior\n\ndef evaluate_model(env, agent, max_steps=50, save_prefix='test'):\n    \"\"\"Evaluate the model on an environment and visualize the results\"\"\"\n    state, _ = env.reset()\n    frames = []\n    \n    for step in range(max_steps):\n        action = agent.select_action(state, evaluate=True)\n        next_state, reward, done, truncated, info = env.step(action)\n        \n        # Record frame\n        fig = env.render()\n        frames.append(fig)\n        plt.close(fig)\n        \n        state = next_state\n        print(f\"Step {step}: Reward = {reward:.2f}, Valid = {info.get('valid', False)}\")\n        \n        if done or truncated:\n            print(f\"Completed in {step+1} steps with {len(env.elements)} elements\")\n            break\n    \n    # Save the last frame\n    if frames:\n        frames[-1].savefig(f'{save_prefix}_final_mesh.png')\n    \n    # Try to create a GIF\n    try:\n        import imageio\n        from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n        \n        print(f\"Creating {save_prefix} mesh generation GIF...\")\n        images = []\n        for fig in frames:\n            canvas = FigureCanvas(fig)\n            canvas.draw()\n            img = np.array(canvas.renderer.buffer_rgba())\n            images.append(img)\n            plt.close(fig)\n        \n        imageio.mimsave(f'{save_prefix}_mesh_generation.gif', images, fps=2)\n        print(f\"✅ Saved: {save_prefix}_mesh_generation.gif\")\n    except ImportError:\n        print(\"⚠️ imageio not installed — skipping animation export.\")\n    \n    return len(env.elements)\n\ndef main():\n    # Create output directory\n    os.makedirs('output', exist_ok=True)\n    \n    # Set random seed for reproducibility\n    np.random.seed(42)\n    \n    # Create training domain\n    print(\"Creating training domain...\")\n    boundary, interior = create_grid_domain(width=3, height=3, interior_points=2)\n    env = MeshEnvironment(initial_boundary=boundary, interior_points=interior)\n    \n    # Visualize initial domain\n    fig = env.plot_domain()\n    plt.savefig('output/initial_domain.png')\n    plt.close(fig)\n    \n    # Setup agent\n    print(\"Setting up RL agent...\")\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.shape[0]\n    agent = SAC(state_dim=state_dim, action_dim=action_dim, hidden_dim=128)\n    replay_buffer = ReplayBuffer(capacity=100_000)\n    \n    # Training configuration\n    total_timesteps = 50_000\n    batch_size = 256\n    initial_random_steps = 5_000\n    eval_interval = 5_000\n    \n    # Create trainer\n    trainer = SACTrainer(\n        env=env,\n        agent=agent,\n        replay_buffer=replay_buffer,\n        total_timesteps=total_timesteps,\n        batch_size=batch_size,\n        initial_random_steps=initial_random_steps,\n        eval_interval=eval_interval,\n        max_ep_len=50\n    )\n    \n    # Training\n    print(f\"Starting training for {total_timesteps} timesteps...\")\n    start_time = time.time()\n    results = trainer.train()\n    training_time = time.time() - start_time\n    print(f\"Training completed in {training_time:.2f} seconds\")\n    \n    # Plot training results\n    plot_training_results(results)\n    plt.savefig('output/training_results.png')\n    \n    # Evaluate on training domain\n    print(\"\\nEvaluating on training domain...\")\n    n_elements = evaluate_model(env, agent, max_steps=50, save_prefix='output/training')\n    \n    # Test on a slightly scaled up version\n    print(\"\\nTesting on scaled up domain...\")\n    boundary_scaled, interior_scaled = create_grid_domain(width=4, height=4, interior_points=3)\n    env_scaled = MeshEnvironment(initial_boundary=boundary_scaled, interior_points=interior_scaled)\n    \n    # Visualize initial scaled domain\n    fig = env_scaled.plot_domain()\n    plt.savefig('output/scaled_initial_domain.png')\n    plt.close(fig)\n    \n    # Evaluate on scaled domain\n    n_elements_scaled = evaluate_model(env_scaled, agent, max_steps=100, save_prefix='output/scaled')\n    \n    # Print summary\n    print(\"\\n===== Summary =====\")\n    print(f\"Training domain: {n_elements} elements created\")\n    print(f\"Scaled domain: {n_elements_scaled} elements created\")\n    print(f\"Training time: {training_time:.2f} seconds\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T02:37:01.186467Z","iopub.execute_input":"2025-04-14T02:37:01.187046Z","iopub.status.idle":"2025-04-14T03:03:50.939054Z","shell.execute_reply.started":"2025-04-14T02:37:01.187013Z","shell.execute_reply":"2025-04-14T03:03:50.938077Z"}},"outputs":[{"name":"stdout","text":"Creating training domain...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"Setting up RL agent...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1744598222.584214      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1744598222.584899      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Starting training for 50000 timesteps...\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nTimestep: 5000 | Actor Loss: -1.6856 | Critic Loss: 2.8232 | Alpha: 1.0000\nEvaluation at timestep 5000: -5.00\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nTimestep: 6000 | Actor Loss: -9.6599 | Critic Loss: 0.0256 | Alpha: 0.7407\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nTimestep: 7000 | Actor Loss: -14.1166 | Critic Loss: 0.0115 | Alpha: 0.5487\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nTimestep: 8000 | Actor Loss: -17.0869 | Critic Loss: 0.0088 | Alpha: 0.4065\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nTimestep: 9000 | Actor Loss: -18.8426 | Critic Loss: 0.0062 | Alpha: 0.3011\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nTimestep: 10000 | Actor Loss: -19.7121 | Critic Loss: 0.0020 | Alpha: 0.2231\nEvaluation at timestep 10000: -5.00\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nTimestep: 11000 | Actor Loss: -19.9426 | Critic Loss: 0.0012 | Alpha: 0.1653\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\nEpisode Return: -5.00 | Elements: 0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3787906652.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_31/3787906652.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting training for {total_timesteps} timesteps...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training completed in {training_time:.2f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1352537631.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_random_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mtrain_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/631383314.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Compute Q-values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mq1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    860\u001b[0m                 \u001b[0marg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_arguments_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0monly_tensor_arg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_arguments_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m                 mask = tree.map_structure(\n\u001b[0m\u001b[1;32m    863\u001b[0m                     \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m                     \u001b[0monly_tensor_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/tree/tree_api.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mstructure\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mlayout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \"\"\"\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/tree/optree_impl.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstructures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     return optree.tree_map(\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_is_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"keras\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optree/ops.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mat\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mrests\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \"\"\"\n\u001b[0;32m--> 764\u001b[0;31m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_is_leaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11}]}